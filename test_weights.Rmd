---
title: "Test weights"
author: "Mirjam R. Rieger"
date: "2024-11-12"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, fig.width = 12, fig.height = 4)
library(tidyverse)
library(brms)
library(svDialogs)
library(parallel)
if (!"EAS" %in% installed.packages()) {
  devtools::install_github("m-rieger/EAS")
}
library(EAS)
csr <- require(cmdstanr) # for core/chain parallelisation, if not installed, chains will not be parallelized and it will take more time
# check out readme for instructions on how to use cmdstanr and cmdstan if you want to use it
if(csr) cs <- try(cmdstan_version(), silent = TRUE) # cmdstan installed?
if("try-error" %in% class(cs)) {
  csr <- FALSE
}
if(csr) check_cmdstan_toolchain(fix = TRUE) # toolchain set up properly?

std <- function(x) sd(x)/sqrt(length(x))

## variables
MODEL <- FALSE # rerun models?
scheme <- "identical - no trend"

nc <- 4 # no of chains

if (csr == TRUE) {
  backend <- "cmdstanr"
  thread  <- floor((detectCores()-1)/2)
  ncores  <- 2
}

if (csr == FALSE) {
  backend <- "rstan"
  ncores  <- min(detectCores()-1, nc)
  thread  <- NULL
}

```


```{r}
## ideas for better simulation
# only 20 sites per year (all the time)
  # do this per repetition: same dataset once for 0:20 - 20:0 ratio
  # other dataset for 2nd repetition (for all ratios once)
  # other dataset for 3rd repetition, ...



## simulate data
nsite <- 20 # number of sites per year and region, this does not really matter
nyear <-  10 # seems like 10 is the minimum number of years to use s() https://stat.ethz.ch/pipermail/r-sig-ecology/2011-May/002148.html
niter <- 20 # how often should the models be rerun
# N <- rpois(nsite*nyear, 5)
# Y <- round(runif(nsite*nyear, min = 0.5, max = nyear+0.5))
# R <- c(rep("A", nsite*nyear/2), rep("B", nsite*nyear/2))
# ggplot(mapping = aes(x = Y, y = N, color = R)) +
#     geom_point(alpha = 0.5) +
#     theme_light()
# df <- data.frame(N = N, Y = Y, R = as.factor(R))

# mod <- brm(N ~ s(Y, by = R) + R,
#            data = df,
#            family = poisson(link = "log"),
#            cores = 4, iter = 1000, warmup = 500, chains = 4,
#            thin = 1) 

# mod <- update(mod, newdata = df)
# summary(mod)

## simulate data
df <- expand.grid(Y = seq(1:nyear),
                  R = c("A", "B"),
                  FID = seq(1:nsite))
df <- df %>% group_by(Y, R) %>%
  mutate(N = rpois(n(), 5)) %>%
  ungroup()

df$ID <- paste0(df$R, ".", df$FID)

# df <- data.frame(year = c(rep(1, 60), rep(2, 60), rep(3, 60), rep(4, 60), rep(5, 60), rep(6, 60), rep(7, 60)),
#                  habitat = c(rep("A", 20), rep("B", 20), rep("C", 20), 
#                              rep("A", 20), rep("B", 20), rep("C", 20), 
#                              rep("A", 20), rep("B", 20), rep("C", 20), 
#                              rep("A", 20), rep("B", 20), rep("C", 20), 
#                              rep("A", 20), rep("B", 20), rep("C", 20), 
#                              rep("A", 20), rep("B", 20), rep("C", 20), 
#                              rep("A", 20), rep("B", 20), rep("C", 20)),
#                  region = rep(c(rep("atl", 30), rep("kon", 30)), 7),
#                  Ntmp = c(rep(c(rpois(20, 5), rpois(20, 10), rpois(20, 15)), 7)),
#                  FID = c(rep(1:20, 7*3)))


df2a <- df[df$Y == 2 & df$R == "A",]
df2b <- df[df$Y == 2 & df$R == "B",]

## get df.habitat (for weights)
df.habitat <- data.frame(R = c("A", "B", "C"),
                         area = c(100, 100, 100)) ## eqally distributed
df.habitat <- data.frame(R = as.factor(c("A", "B")),
                         area = c(100, 100)) ## eqally distributed

```

# schemes

- **identical - no trend**: both regions have identical data (same mean abundance) with no trend across years.  



# simulated data

Histogram of simulated abundance data (per year and region). The used scheme is `r scheme`.  
```{r}
ggplot(df) + geom_histogram(aes(N), binwidth = 1) + 
  facet_wrap(~paste0("year ", Y, ", region ",R)) +
  theme_light()

```

Abundance per year and region. The used scheme is `r scheme`.  
```{r}
ggplot(df, aes(x = Y, y = jitter(N, 0.2), color = R, group = R)) +
  geom_point(alpha = 0.5, position = position_dodge(width = 0.5), pch = 1, size = 3) +
  scale_color_viridis_d(end = 0.8, option = "rocket") +
  scale_x_continuous(breaks = seq(1,nyear, 1)) +
  ylab("N") +
  theme_light()
```


```{r model}

## define model formula
mod.form <- bf(N|weights(weight, scale = FALSE) ~s(Y, by = R) + R) # apparently not working for GAM prob. not unique enough data
mod.form.ow <- bf(N ~s(Y, by = R) + R) #

# mod.form <- bf(N|weights(weight, scale = FALSE) ~Y + R)
# mod.form.ow <- bf(N ~Y + R)

fam <- poisson(link = "log")

## empty files for storing data
newdat.total <- NULL

if(MODEL) {
  ## create dataset for each repetition
  for(rep in 1:niter) {
    
    ## get dataset (20 sites per year, ratio 10:10)
    df2 <- df[df$Y != 2,] %>% group_by(Y, R) %>%
      mutate(keep = sample(c(rep(FALSE, nsite/2), rep(TRUE, nsite/2)))) %>% ## random vector with TRUE and FALSE 1:1
      ungroup()
    
    df2 <- select(df2[df2$keep,], -c("keep"))
  
    for(i in 0:nsite) { # repeat model for different ratios in year 2
      ## add sample from year 2
      tmp <- rbind(df2, df2a[sample(1:nsite, size = i),])
      tmp <- rbind(tmp, df2b[sample(1:nsite, size = nsite-i),])
      
      tmp <- as.data.frame(tmp)
      
      tmp <- weight(df.habitat = df.habitat,   # landscape data
                   habitat = "R", # column name for habitat
                   area = "area",     # column name for area
                   df.data = tmp,         # raw data
                   ID = "ID",             # default column name
                   year = "Y",         # default column name
                   by_spec = F,        # calculate weight by species (e.g., if number of sites differs between species due to exclusion)
                   by_reg = FALSE,        # default
                   add.df = TRUE)         # default: returns an additional dataframe with weights per habitat and year (and species)
    
      tmp$R <- as.factor(tmp$R)
      tmp$ID <- as.factor(tmp$ID)
      
    
      priors <- get_prior(mod.form,
                            data = tmp, family = fam)
      priors$prior[1] <- "normal(0, 2.5)" 
      
      if(rep == 1 & i == 0) {
        mod <- brm(mod.form,
                   data = tmp, family = fam,
                   cores = ncores, iter = 1000, warmup = 500, chains = nc,
                   backend = backend, threads = threading(thread),
                   thin = 1,
                   prior = priors)       
      }
      
      else mod <- update(mod, newdata = tmp, recompile = FALSE)
    
      #summary(mod)
      
      ## create newdat
      newdat <- expand.grid(Y = seq(min(tmp$Y), max(tmp$Y), length = max(tmp$Y)),
                          R = levels(tmp$R))
    
      ## extract predictions and newdat in lists, add overall predictions weighted by proportion of atl and kon in NRW
      pred.list   <- NULL
      newdat.list <- NULL
    
      pred.list[["A"]]       <- as.data.frame(t(fitted(mod, newdata = newdat,  summary = FALSE, re_formula = NA))[newdat$R == "A",])
      newdat.list[["A"]]     <- newdat[newdat$R == "A",]
    
    
      pred.list[["B"]]       <- as.data.frame(t(fitted(mod, newdata = newdat,  summary = FALSE, re_formula = NA))[newdat$R == "B",])
      newdat.list[["B"]]     <- newdat[newdat$R == "B",]
    
    
      pred.list[["overall"]]   <- pred.list[["A"]]*0.5 + pred.list[["B"]]*0.5 
      newdat.list[["overall"]] <- newdat.list[["B"]]
      newdat.list[["overall"]]$R <- "overall"
      
      ## loop through regions
      for (r in names(pred.list)) {
        pred.r   <- pred.list[[r]]
        newdat.r <- newdat.list[[r]]
    
        ## trend & 95% CrI
        ##################-
        newdat.r$lwr <- apply(X = pred.r, MARGIN = 1, FUN = quantile, prob = 0.025)
        newdat.r$fit <- apply(X = pred.r, MARGIN = 1, FUN = quantile, prob = 0.5)
        newdat.r$upr <- apply(X = pred.r, MARGIN = 1, FUN = quantile, prob = 0.975)
        newdat.r$se <- apply(X = pred.r, MARGIN = 1, FUN = std)
        newdat.r$sd <- apply(X = pred.r, MARGIN = 1, FUN = sd)
  
        newdat.r$noA <- i
        newdat.r$rep <- rep
        newdat.r$weight <- "yes"
        
        ## merge data
        newdat.total     <- rbind(newdat.total, newdat.r)
    
      } ## end of region loop
      
      
      if(rep == 1 & i == 0) {
        mod.ow <- brm(mod.form.ow,
                   data = tmp, family = fam,
                   cores = ncores, iter = 1000, warmup = 500, chains = nc,
                   backend = backend, threads = threading(thread),
                   thin = 1,
                   prior = priors,
                   refresh = 0)       
      }
      
      else mod.ow <- update(mod.ow, newdata = tmp, recompile = FALSE)
  
    
      #summary(mod)
      
      ## create newdat
      newdat <- expand.grid(Y = seq(min(tmp$Y), max(tmp$Y), length = max(tmp$Y)),
                          R = levels(tmp$R))
    
        ## extract predictions and newdat in lists, add overall predictions weighted by proportion of atl and kon in NRW
      pred.list   <- NULL
      newdat.list <- NULL
    
      pred.list[["A"]]       <- as.data.frame(t(fitted(mod.ow, newdata = newdat,  summary = FALSE, re_formula = NA))[newdat$R == "A",])
      newdat.list[["A"]]     <- newdat[newdat$R == "A",]
    
    
      pred.list[["B"]]       <- as.data.frame(t(fitted(mod.ow, newdata = newdat,  summary = FALSE, re_formula = NA))[newdat$R == "B",])
      newdat.list[["B"]]     <- newdat[newdat$R == "B",]
    
    
      pred.list[["overall"]]   <- pred.list[["A"]]*0.5 + pred.list[["B"]]*0.5 
      newdat.list[["overall"]] <- newdat.list[["B"]]
      newdat.list[["overall"]]$R <- "overall"
      
      ## loop through regions
      for (r in names(pred.list)) {
        pred.r   <- pred.list[[r]]
        newdat.r <- newdat.list[[r]]
    
        ## trend & 95% CrI
        ##################-
        newdat.r$lwr <- apply(X = pred.r, MARGIN = 1, FUN = quantile, prob = 0.025)
        newdat.r$fit <- apply(X = pred.r, MARGIN = 1, FUN = quantile, prob = 0.5)
        newdat.r$upr <- apply(X = pred.r, MARGIN = 1, FUN = quantile, prob = 0.975)
        newdat.r$se <- apply(X = pred.r, MARGIN = 1, FUN = std)
        newdat.r$sd <- apply(X = pred.r, MARGIN = 1, FUN = sd)
    
        newdat.r$noA <- i
        newdat.r$rep <- rep
        newdat.r$weight <- "no"
  
        ## merge data
        newdat.total     <- rbind(newdat.total, newdat.r)
    
      } ## end of region loop
      
    } # end of ratio loop
  
  
  write.csv(newdat.total, paste0("test_weights_", scheme, ".csv"), row.names = F)
  
    
  } # end of iter loop
  
  write.csv(newdat.total, paste0("test_weights_", scheme, ".csv"), row.names = F)
}

if(!MODEL) newdat.total <- read.csv(paste0("test_weights_", scheme, ".csv"))


```

# Model output
Data was simulated with  `r scheme`, `r nsite` sites per region (A and B) and `r nyear` years in total. Each year, only `r nsite` sites were included in the analysis (`r nsite/2` per region). For the second year (Y = 2), the ratio of A:B ranged from 0:`r nsite` to `r nsite`:0, the target ratio would be 1:1. Each ratio was repeated `r niter` times, each repetition randomly selected new sites. For each round of varying ratios from `r nsite`:0 til 0:`r nsite`, the randomly selected data of other years remained the same (resulting in `r niter` different datasets which were each used for different ratios). The model formula was N ~ s(Y, by = R) + R, with N = abundance, Y = year, R = region, family = Poisson. Weighted models additionally added a weight to N. The respective weight for A (or B) in year 2 can be calculated as 0.5/(No. of A (or B) sites in year 2/`r nsite`), the number of B sites is `r nsite` - No. of A sites in year 2.   

## annual parameters {.tabset}
Only year 1-5 are included with year 2 being the most important comparison as this is the only year where the regions got different weights.  

### Standard error

```{r}
for(y in 1:5) {
  print(ggplot(newdat.total[newdat.total$Y == y,], aes(x = as.factor(noA), y = se, color = weight)) +
    geom_boxplot() +
    facet_wrap(~R) +
    ggtitle(paste0("se for year = ", y)) +
      ylab("se of post. predictions") +
      xlab("No. of A sites in year 2") +
      scale_color_viridis_d(end = 0.8, option = "rocket") +
    theme_light())
  
}

```

### Standard deviation

```{r}
for(y in 1:5) {
  print(ggplot(newdat.total[newdat.total$Y == y,], aes(x = as.factor(noA), y = sd, color = weight)) +
    geom_boxplot() +
    facet_wrap(~R) +
    ggtitle(paste0("sd for year = ", y)) +
      ylab("sd of post. predictions") +
      xlab("No. of A sites in year 2") +
      scale_color_viridis_d(end = 0.8, option = "rocket") +
    theme_light())
  
}

```

### Model fit (median abundance)

```{r}
for(y in 1:5) {
  print(ggplot(newdat.total[newdat.total$Y == y,], aes(x = as.factor(noA), y = fit, color = weight)) +
    geom_boxplot() +
    facet_wrap(~R) +
    ggtitle(paste0("median for year = ", y)) +
      ylab("fit of post. predictions") +
      xlab("No. of A sites in year 2") +
      scale_color_viridis_d(end = 0.8, option = "rocket") +
    theme_light())
  
}

```

## trends

```{r}
for(i in unique(newdat.total$noA)) {
  print(ggplot(newdat.total[newdat.total$noA == i,], 
               aes(x = Y, color = weight, group = paste0(rep, weight))) +
    geom_line(aes(y = fit), alpha = 0.5) +
    geom_line(aes(y = lwr), alpha = 0.2) +
    geom_line(aes(y = upr), alpha = 0.2) +
    facet_wrap(~R) +
    ggtitle(paste0("trend for no. of A sites in year 2 = ", i, " (weight for A = ", round(nsite/2/i, 2), ", weight for B = ", round(nsite/2/(nsite-i), 2), ")")) +
      ylab("abundance (fit incl. 95% CrI)") +
      xlab("year") +
      scale_color_viridis_d(end = 0.8, option = "rocket") +
      scale_x_continuous(breaks = seq(1,nyear, 1)) +
    theme_light())
  
}

```

